//! Unified Inference Orchestrator — Core Types
//!
//! This module defines the **provider-agnostic data contract** for the MoFA inference
//! orchestration layer. Every type here is deliberately decoupled from any single
//! backend (OpenAI, Anthropic, local Candle, MLX, etc.) so that the [`InferenceBackend`]
//! trait can operate over a single, normalized schema.
//!
//! ## MoFA Microkernel Alignment
//!
//! In the MoFA microkernel architecture, the **foundation layer** provides stable
//! abstractions that higher layers (runtime, SDK, plugins) depend on. These types
//! form the "system call" boundary for inference — any agent, workflow, or pipeline
//! can construct an [`InferenceRequest`] and receive an [`InferenceResponse`] without
//! knowing whether the tokens were generated locally on Apple Silicon or streamed
//! from a cloud API.
//!
//! ## Streaming Model
//!
//! For latency-sensitive applications (chat UIs, voice assistants), the orchestrator
//! yields individual [`Token`] values through a pinned async stream. Each token
//! carries its generated text fragment and an optional log-probability for downstream
//! quality monitoring.

use serde::{Deserialize, Serialize};
use thiserror::Error;

// ============================================================================
// Chat Message Types
// ============================================================================

/// Role of a participant in a chat conversation.
///
/// This enum is intentionally minimal and provider-agnostic. Backend adapters
/// (e.g., [`CloudOpenAIProvider`](super::cloud_openai::CloudOpenAIProvider)) map these roles to their provider-specific
/// equivalents during request conversion.
#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum ChatRole {
    /// System-level instructions that guide the model's behavior.
    /// Typically set once at the beginning of a conversation.
    System,
    /// A message from the human user.
    User,
    /// A message generated by the AI assistant.
    Assistant,
}

/// A single message in a chat conversation.
///
/// Messages form an ordered sequence that provides context to the model.
/// The orchestrator preserves message ordering when forwarding to backends.
///
/// # Example
/// ```
/// use mofa_foundation::orchestrator::types::{ChatMessage, ChatRole};
///
/// let messages = vec![
///     ChatMessage::system("You are a helpful assistant."),
///     ChatMessage::user("What is Rust?"),
/// ];
/// ```
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    /// The role of the message author.
    pub role: ChatRole,
    /// The text content of the message.
    pub content: String,
}

impl ChatMessage {
    /// Create a new message with the given role and content.
    pub fn new(role: ChatRole, content: impl Into<String>) -> Self {
        Self {
            role,
            content: content.into(),
        }
    }

    /// Shorthand: create a **system** message.
    pub fn system(content: impl Into<String>) -> Self {
        Self::new(ChatRole::System, content)
    }

    /// Shorthand: create a **user** message.
    pub fn user(content: impl Into<String>) -> Self {
        Self::new(ChatRole::User, content)
    }

    /// Shorthand: create an **assistant** message.
    pub fn assistant(content: impl Into<String>) -> Self {
        Self::new(ChatRole::Assistant, content)
    }
}

// ============================================================================
// Inference Request
// ============================================================================

/// A normalized inference request that can be dispatched to any backend.
///
/// This struct captures the full set of generation parameters that are common
/// across cloud APIs and local inference engines. Backend-specific parameters
/// (e.g., OpenAI's `presence_penalty`, Candle's `repeat_penalty`) are intentionally
/// excluded — they belong in the backend's own configuration.
///
/// ## Design Rationale
///
/// The request is **immutable by convention**: backends receive a shared reference
/// (`&InferenceRequest`) so that the same request can be retried, logged, or
/// dispatched to multiple backends without cloning.
///
/// # Example
/// ```
/// use mofa_foundation::orchestrator::types::{ChatMessage, InferenceRequest};
///
/// let request = InferenceRequest::new(vec![
///     ChatMessage::system("You are a coding assistant."),
///     ChatMessage::user("Explain async/await in Rust"),
/// ])
/// .with_temperature(0.7)
/// .with_max_tokens(1024);
/// ```
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceRequest {
    /// Ordered conversation messages. Must contain at least one message.
    pub messages: Vec<ChatMessage>,

    /// Target model identifier (e.g., `"gpt-4o"`, `"llama-3.1-8b"`).
    /// When `None`, the backend uses its configured default model.
    pub model: Option<String>,

    /// Sampling temperature in `[0.0, 2.0]`.
    /// Lower values → more deterministic; higher → more creative.
    /// `None` uses the backend's default (typically `1.0`).
    pub temperature: Option<f32>,

    /// Maximum number of tokens to generate.
    /// Backends clamp this to their model's context limit.
    pub max_tokens: Option<u32>,

    /// Nucleus sampling threshold in `[0.0, 1.0]`.
    /// Only tokens with cumulative probability ≤ `top_p` are considered.
    pub top_p: Option<f32>,

    /// Stop sequences — generation halts when any of these strings is produced.
    pub stop_sequences: Option<Vec<String>>,
}

impl InferenceRequest {
    /// Create a new request with the given messages and default parameters.
    pub fn new(messages: Vec<ChatMessage>) -> Self {
        Self {
            messages,
            model: None,
            temperature: None,
            max_tokens: None,
            top_p: None,
            stop_sequences: None,
        }
    }

    /// Set the target model.
    pub fn with_model(mut self, model: impl Into<String>) -> Self {
        self.model = Some(model.into());
        self
    }

    /// Set the sampling temperature.
    pub fn with_temperature(mut self, temperature: f32) -> Self {
        self.temperature = Some(temperature);
        self
    }

    /// Set the maximum number of tokens to generate.
    pub fn with_max_tokens(mut self, max_tokens: u32) -> Self {
        self.max_tokens = Some(max_tokens);
        self
    }

    /// Set the nucleus sampling threshold.
    pub fn with_top_p(mut self, top_p: f32) -> Self {
        self.top_p = Some(top_p);
        self
    }

    /// Set stop sequences.
    pub fn with_stop_sequences(mut self, stop: Vec<String>) -> Self {
        self.stop_sequences = Some(stop);
        self
    }
}

// ============================================================================
// Token (Streaming Unit)
// ============================================================================

/// A single token emitted during streaming generation.
///
/// Tokens are the atomic unit of the streaming interface. Each token carries
/// a text fragment (which may be a partial word, full word, or punctuation)
/// and an optional log-probability for quality/confidence monitoring.
///
/// ## Streaming Contract
///
/// The [`InferenceBackend::stream`](super::backend::InferenceBackend::stream) method yields these tokens through a
/// `Pin<Box<dyn Stream<Item = Result<Token, InferenceError>> + Send>>`.
/// Consumers concatenate the `text` fields to reconstruct the full response.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Token {
    /// The text fragment for this token (e.g., `"Hello"`, `","`, `" world"`).
    pub text: String,

    /// Log-probability of this token, if the backend supports it.
    /// Useful for confidence scoring and quality monitoring.
    pub logprob: Option<f32>,
}

impl Token {
    /// Create a new token with just text content.
    pub fn new(text: impl Into<String>) -> Self {
        Self {
            text: text.into(),
            logprob: None,
        }
    }

    /// Create a new token with text and log-probability.
    pub fn with_logprob(text: impl Into<String>, logprob: f32) -> Self {
        Self {
            text: text.into(),
            logprob: Some(logprob),
        }
    }
}

// ============================================================================
// Token Usage
// ============================================================================

/// Token usage statistics for a completed inference request.
///
/// Backends populate this from their provider-specific usage data.
/// This is critical for cost tracking, quota management, and telemetry
/// in the MoFA monitoring layer.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct TokenUsage {
    /// Number of tokens in the input prompt.
    pub prompt_tokens: u32,
    /// Number of tokens generated in the completion.
    pub completion_tokens: u32,
    /// Total tokens consumed (`prompt_tokens + completion_tokens`).
    pub total_tokens: u32,
}

// ============================================================================
// Inference Response
// ============================================================================

/// A normalized inference response returned by any backend.
///
/// This struct contains the complete generation result, including the model
/// identifier, token usage, and finish reason. It is returned by
/// [`InferenceBackend::generate`](super::backend::InferenceBackend::generate) for non-streaming requests.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceResponse {
    /// The generated text content.
    pub content: String,

    /// The model that produced this response (as reported by the backend).
    pub model: String,

    /// Token usage statistics for billing and monitoring.
    pub usage: TokenUsage,

    /// Why generation stopped: `"stop"`, `"length"`, `"content_filter"`, etc.
    /// `None` if the backend does not report a finish reason.
    pub finish_reason: Option<String>,
}

// ============================================================================
// Inference Error
// ============================================================================

/// Unified error type for the inference orchestration layer.
///
/// This enum covers all failure modes across local and cloud backends,
/// providing a single error surface for callers. Each variant maps to
/// a specific category of failure, enabling intelligent retry policies
/// and fallback strategies in the orchestrator.
///
/// ## Error Handling Strategy
///
/// | Variant | Retryable? | Typical Action |
/// |---------|-----------|----------------|
/// | `RateLimited` | Yes | Exponential backoff |
/// | `ProviderError` | Maybe | Retry on 5xx, fail on 4xx |
/// | `BackendUnavailable` | Yes | Fallback to next backend |
/// | `StreamError` | Yes | Reconnect and resume |
/// | `Timeout` | Yes | Retry with extended deadline |
/// | `ConfigError` | No | Fix configuration |
/// | `Other` | No | Report and fail |
#[derive(Debug, Clone, Error)]
pub enum InferenceError {
    /// The backend is not available (not initialized, GPU offline, API unreachable).
    /// The orchestrator should attempt fallback to an alternative backend.
    #[error("Backend unavailable: {0}")]
    BackendUnavailable(String),

    /// Invalid or missing configuration (API key, model ID, base URL).
    /// This is a non-retryable error — the caller must fix the configuration.
    #[error("Configuration error: {0}")]
    ConfigError(String),

    /// Rate-limited by the provider (HTTP 429).
    /// The orchestrator should apply exponential backoff with jitter.
    #[error("Rate limited: {0}")]
    RateLimited(String),

    /// Provider returned an error (HTTP 4xx/5xx, inference failure).
    /// May be retryable depending on the status code.
    #[error("Provider error: {0}")]
    ProviderError(String),

    /// Error during token streaming (connection drop, malformed chunk).
    /// The stream should be re-established if possible.
    #[error("Stream error: {0}")]
    StreamError(String),

    /// Request timed out before completion.
    #[error("Request timed out: {0}")]
    Timeout(String),

    /// Catch-all for unclassified errors.
    #[error("Inference error: {0}")]
    Other(String),
}

/// Convenient Result alias for inference operations.
pub type InferenceResult<T> = Result<T, InferenceError>;

// ============================================================================
// Tests
// ============================================================================

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_chat_message_constructors() {
        let sys = ChatMessage::system("You are helpful.");
        assert_eq!(sys.role, ChatRole::System);
        assert_eq!(sys.content, "You are helpful.");

        let user = ChatMessage::user("Hello");
        assert_eq!(user.role, ChatRole::User);

        let asst = ChatMessage::assistant("Hi there!");
        assert_eq!(asst.role, ChatRole::Assistant);
    }

    #[test]
    fn test_inference_request_builder() {
        let req = InferenceRequest::new(vec![
            ChatMessage::system("System prompt"),
            ChatMessage::user("User input"),
        ])
        .with_model("gpt-4o")
        .with_temperature(0.7)
        .with_max_tokens(1024)
        .with_top_p(0.95)
        .with_stop_sequences(vec!["STOP".into()]);

        assert_eq!(req.messages.len(), 2);
        assert_eq!(req.model.as_deref(), Some("gpt-4o"));
        assert_eq!(req.temperature, Some(0.7));
        assert_eq!(req.max_tokens, Some(1024));
        assert_eq!(req.top_p, Some(0.95));
        assert_eq!(req.stop_sequences.as_ref().unwrap().len(), 1);
    }

    #[test]
    fn test_inference_request_defaults() {
        let req = InferenceRequest::new(vec![ChatMessage::user("test")]);
        assert!(req.model.is_none());
        assert!(req.temperature.is_none());
        assert!(req.max_tokens.is_none());
        assert!(req.top_p.is_none());
        assert!(req.stop_sequences.is_none());
    }

    #[test]
    fn test_token_creation() {
        let tok = Token::new("Hello");
        assert_eq!(tok.text, "Hello");
        assert!(tok.logprob.is_none());

        let tok_lp = Token::with_logprob("world", -0.5);
        assert_eq!(tok_lp.text, "world");
        assert_eq!(tok_lp.logprob, Some(-0.5));
    }

    #[test]
    fn test_token_usage_default() {
        let usage = TokenUsage::default();
        assert_eq!(usage.prompt_tokens, 0);
        assert_eq!(usage.completion_tokens, 0);
        assert_eq!(usage.total_tokens, 0);
    }

    #[test]
    fn test_inference_error_display() {
        let err = InferenceError::RateLimited("retry after 30s".into());
        assert!(err.to_string().contains("Rate limited"));

        let err = InferenceError::BackendUnavailable("GPU offline".into());
        assert!(err.to_string().contains("Backend unavailable"));

        let err = InferenceError::ConfigError("missing API key".into());
        assert!(err.to_string().contains("Configuration error"));
    }

    #[test]
    fn test_chat_message_serialization_roundtrip() {
        let msg = ChatMessage::user("Hello, world!");
        let json = serde_json::to_string(&msg).unwrap();
        let deserialized: ChatMessage = serde_json::from_str(&json).unwrap();
        assert_eq!(deserialized.role, ChatRole::User);
        assert_eq!(deserialized.content, "Hello, world!");
    }

    #[test]
    fn test_inference_request_serialization() {
        let req = InferenceRequest::new(vec![ChatMessage::user("test")])
            .with_model("gpt-4o")
            .with_temperature(0.5);

        let json = serde_json::to_string(&req).unwrap();
        assert!(json.contains("gpt-4o"));
        assert!(json.contains("0.5"));

        let deserialized: InferenceRequest = serde_json::from_str(&json).unwrap();
        assert_eq!(deserialized.model.as_deref(), Some("gpt-4o"));
    }

    #[test]
    fn test_inference_response_structure() {
        let resp = InferenceResponse {
            content: "Hello!".into(),
            model: "gpt-4o".into(),
            usage: TokenUsage {
                prompt_tokens: 10,
                completion_tokens: 5,
                total_tokens: 15,
            },
            finish_reason: Some("stop".into()),
        };

        assert_eq!(resp.content, "Hello!");
        assert_eq!(resp.usage.total_tokens, 15);
        assert_eq!(resp.finish_reason.as_deref(), Some("stop"));
    }
}
